ğŸ§© Databricks Declarative Pipelines (Delta Live Tables)

This project demonstrates the implementation of Databricks Declarative Pipelines, also known as Delta Live Tables (DLT) â€” a modern framework for building reliable, scalable, and automated data pipelines on the Databricks Lakehouse Platform.
The goal of this project is to design and orchestrate a streaming data pipeline that seamlessly transforms raw data into clean, enriched, and analytics-ready datasets following the Medallion Architecture (Bronze â†’ Silver â†’ Gold).

ğŸ“ Folder Structure
DeclarativePipelines/
â”‚
â”œâ”€â”€ transformations_SourceCode/
â”‚   â”œâ”€â”€ bronze/      # Raw ingestion layer
â”‚   â”œâ”€â”€ silver/      # Cleansed and enriched data
â”‚   â””â”€â”€ gold/        # Aggregated business-level tables
â”‚
â”œâ”€â”€ notebooks/       # DLT notebooks defining views and tables
â””â”€â”€ configs/         # DLT pipeline configurations

ğŸ§  Key Concepts
ğŸ§± Streaming Tables and Materialized Views
âš™ï¸ Auto CDC (Change Data Capture) and Slowly Changing Dimensions (SCD Type 1 & 2)
ğŸ“Š Data Quality Checks & Expectations
ğŸ”„ Append Flows and Incremental Processing
ğŸš€ End-to-End ETL Pipeline with DLT Orchestration

ğŸ—ï¸ Tech Stack
Databricks | Delta Live Tables | PySpark
Structured Streaming | Auto Loader
Delta Lake | SQL | Medallion Architecture
